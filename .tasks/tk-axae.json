{
  "project": "tk",
  "ref": "axae",
  "title": "[BUG] Kimi k2.5 'Invalid request' errors on OpenRouter",
  "description": null,
  "status": "done",
  "priority": 3,
  "labels": [],
  "assignees": [],
  "parent": null,
  "blocked_by": [
    "tk-mmzs"
  ],
  "estimate": null,
  "due_date": null,
  "logs": [
    {
      "ts": "2026-01-29T05:32:28.227Z",
      "msg": "OpenRouter returns 'Invalid request' for moonshotai/kimi-k2.5. Possibly tool calling issue - model listed as supporting tools but may not handle properly. Error: 'Invalid request: OpenAI: Provider returned error'"
    },
    {
      "ts": "2026-01-29T05:38:08.284Z",
      "msg": "llm-connector ChatRequest lacks 'extra'/'provider' field for OpenRouter routing params. ProviderPrefs exist in ion but can't be sent to API. Known Kimi k2.5 tool calling bugs (GitHub gist). Options: 1) Disable tools for Kimi models, 2) Fork/PR llm-connector for extra_body support, 3) Use different model."
    },
    {
      "ts": "2026-01-29T20:27:41.396Z",
      "msg": "DISTINCT ISSUE: This is about Kimi k2.5 'Invalid request' errors on OpenRouter specifically - root cause unclear, possibly OpenRouter-side bug. Separate from general OpenRouter provider routing field issue (llm-connector limitation). Native Kimi provider provides alternative path but doesn't fix OpenRouter."
    }
  ],
  "created_at": "2026-01-29T05:32:18.963Z",
  "updated_at": "2026-01-29T20:32:36.603Z",
  "completed_at": "2026-01-29T20:32:36.603Z",
  "external": {}
}