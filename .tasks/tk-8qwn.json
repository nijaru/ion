{
  "project": "tk",
  "ref": "8qwn",
  "title": "[RESEARCH] Compare system prompts with other TUI agents",
  "description": null,
  "status": "done",
  "priority": 4,
  "labels": [],
  "assignees": [],
  "parent": null,
  "blocked_by": [],
  "estimate": null,
  "due_date": null,
  "logs": [
    {
      "ts": "2026-02-10T04:48:30.851Z",
      "msg": "Completed: expanded system prompt from ~47 to ~80 lines with Task Execution + Tool Usage sections. Added MCP tool template hint. Surveyed Claude Code, Codex CLI, Gemini CLI patterns. Research in ai/research/system-prompt-survey-2026.md"
    },
    {
      "ts": "2026-02-10T23:34:25.577Z",
      "msg": "2026-02-10: Comprehensive system prompt analysis in ai/research/system-prompt-effectiveness-2026-02.md. 27% reduction possible (860->630 tokens). Task Execution is highest-value section. Codex CLI uses per-model prompts (68 lines for GPT-5 vs 275 for base)."
    }
  ],
  "created_at": "2026-01-19T15:36:19.947Z",
  "updated_at": "2026-02-10T23:34:25.577Z",
  "completed_at": "2026-02-10T04:48:30.828Z",
  "external": {}
}