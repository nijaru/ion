# Competitive Analysis: SOTA Coding Agents (2025)

**Last Updated**: 2025-10-29
**Purpose**: Honest assessment of what competitors do, what we can verify, and where we're guessing

## Evidence Levels

- **âœ… VERIFIED**: Open source code inspected, or official documentation with implementation details
- **âš ï¸ INFERRED**: Public statements, user reports, blog posts, but no code to inspect
- **â“ UNKNOWN**: No public information, pure speculation
- **ðŸ“„ DOCUMENTED**: Official docs/blogs exist but implementation hidden

---

## Open Source Agents (Can Inspect Code)

### 1. OpenCode (SST)
**GitHub**: https://github.com/sst/opencode (28.8k stars)
**Evidence Level**: âœ… VERIFIED (open source, docs available)

**What We Know FOR SURE**:
- âœ… Plan/Build mode separation - DOCUMENTED at https://opencode.ai/docs/modes/
  - Plan mode disables: write, edit, patch, bash tools
  - Build mode: all tools enabled (default)
  - Switch modes with Tab key or configured keybind
- âœ… LSP integration - Confirmed in GitHub issues, configurable via opencode.json
- âœ… Git-based snapshots - Confirmed in GitHub issues (can cause problems with large repos, e.g., 98GB snapshots)
- âœ… Tool configuration per agent - Documented in tools docs
- âœ… Multi-provider support - Anthropic, 75+ LLM providers via Models.dev, local models
- âœ… Terminal UI - Built in Go + TypeScript

**Implementation Details**:
- Language: Go (backend) + TypeScript (frontend)
- Architecture: Terminal-first with IDE integration (VS Code, Cursor)
- Agent system: Mode-based tool restriction, agent-specific prompts
- Configuration: opencode.json for project-specific settings

**Unique Patterns**:
- Mode-based tool permissions (Plan = read-only, Build = full access)
- AGENTS.md file for project structure understanding
- Session sharing via links
- Automatic LSP loading for language support

---

### 2. Zed AI
**GitHub**: https://github.com/zed-industries/zed
**Evidence Level**: âœ… VERIFIED (open source)

**Verified Features**:
- âœ… LSP integration (native, part of editor)
- âœ… Multi-provider support (OpenAI, Anthropic, local)
- âœ… Inline assistant
- âœ… Agent Client Protocol (ACP) support

**Implementation Details**:
- Language: Rust + GPUI
- Architecture: Native LSP integration, no separate event bus
- Agent system: Basic tool calling, no complex orchestration

**Unique Patterns**:
- GPUI framework for native UI
- Direct LSP integration (no abstraction layer needed)
- ACP server/client implementation

---

### 3. Sweep
**GitHub**: https://github.com/sweepai/sweep
**Evidence Level**: âœ… VERIFIED (open source)

**Verified Features**:
- âœ… GitHub integration (issues â†’ PRs)
- âœ… Planning system
- âœ… Multi-step execution
- âœ… Test-driven fixes

**Implementation Details**:
- Language: Python
- Architecture: Planning â†’ Implementation â†’ Testing loop
- Memory: Stores context in GitHub issues/PRs

**Unique Patterns**:
- Issue-driven workflow
- Automatic PR generation
- Test-based validation

---

## Closed Source Agents (Must Infer)

### 4. Claude Code (Anthropic)
**Evidence Level**: ðŸ“„ DOCUMENTED + âš ï¸ INFERRED (official blogs + extensive user feedback)

**What's Documented** (from Anthropic official sources):
- ðŸ“„ "70-80% of the way" success rate (Anthropic blog: "How Anthropic teams use Claude Code")
- ðŸ“„ "Treat it like a slot machine" advice - save state, let run 30 min, accept or restart (official blog)
- ðŸ“„ Sub-agent parallelization feature exists (HN discussion on "How to use Claude Code subagents")
- ðŸ“„ Context management via Claude.md files

**What We Can Verify** (from user reports with consistent patterns):
- âš ï¸ Hidden token/cost data - Anthropic doesn't show token usage in UI (verified by multiple blog posts)
- âš ï¸ 5-hour usage limits cause workflow interruptions (consistent user complaints)
- âš ï¸ Grep-only retrieval strategy - Claude engineer confirmed on HN, no RAG/vector search
  - Result: 40% more token usage vs vector search approaches (Milvus blog post analysis)
- âš ï¸ Rate limit problems during heavy usage
- âš ï¸ No persistent memory across sessions (user requests for feature)
- âš ï¸ No git-based undo (users request feature)

**Performance Data**:
- Terminal-Bench: 43.2% Â± 1.3 (with Claude Opus 4)
- User-reported: "gets 70-80% of the way, then stalls"

**What We DON'T Know**:
- â“ Exact sub-agent architecture (spawning strategy, coordination)
- â“ Internal tool system design
- â“ Context window management specifics

**Sources**:
- Anthropic official blog: "How Anthropic teams use Claude Code"
- HN: Multiple discussions with 500+ comments
- AI Engineering Report: Deep-dive on token usage
- ClaudeLog documentation site
- Milvus blog: Technical analysis of grep vs vector search

**VERIFIED PAIN POINTS**:
1. Cost opacity - users can't optimize without seeing token usage
2. Grep-based search - burns 40% more tokens than semantic alternatives
3. "Slot machine" workflow - success is unpredictable, restart often
4. No long-term memory - repeats research across sessions

---

### 5. Factory Droid (Factory / Droid)
**Evidence Level**: âš ï¸ INFERRED (closed source, benchmark results only)

**What We Can Verify** (from Terminal-Bench):
- âœ… **#2 on Terminal-Bench: 58.8% Â± 0.9** (with Claude Opus 4.1)
- âœ… Previously #1 before Ante overtook at 60.3%
- âš ï¸ Uses "specialized droids" (from marketing materials)
- âš ï¸ Pre-configured prompts per task type (from docs)

**What We DON'T Know**:
- â“ Actual architecture of "droids"
- â“ How droids are implemented vs simple prompt engineering
- â“ Tool system design
- â“ Context management approach
- â“ Whether architecture is fundamentally different or just optimized prompts

**Sources**:
- Terminal-Bench official leaderboard (https://www.tbench.ai/leaderboard)
- Factory website marketing materials
- No technical blog posts, talks, or papers available

**Competitive Position**:
- Strong performance on terminal tasks (58.8%)
- Outperforms Claude Code (43.2%) significantly
- Slightly behind current SOTA Ante (60.3%)

**HONESTY CHECK**: We're assuming "specialized droids" = specialized agent configs. Could be sophisticated prompt engineering, tool selection, or architectural innovation. No way to verify without access.

---

### 6. Sourcegraph Amp (formerly Cody)
**Evidence Level**: ðŸ“„ DOCUMENTED (closed source, but docs exist)

**What's Documented**:
- ðŸ“„ Multi-model routing (Haiku/Sonnet/Opus)
- ðŸ“„ Context fetching from codebase
- ðŸ“„ LLM-based agent system

**What We Can Infer**:
- âš ï¸ Cost-aware model selection (from docs)
- âš ï¸ Task complexity determines model (from docs)

**What We DON'T Know**:
- â“ Implementation details of router
- â“ How complexity is measured
- â“ Actual cost savings

**Sources**:
- Sourcegraph docs
- Blog posts about Cody/Amp

**HONESTY CHECK**: Docs say it routes between models, but no implementation details or real cost numbers.

---

### 7. Cursor
**Evidence Level**: ðŸ“„ DOCUMENTED + âš ï¸ INFERRED (official blog + user reports)

**What's Documented** (from official sources):
- ðŸ“„ **Fast-apply model (Llama-3-70)** - Achieves 1000 tokens/second for code edits (official blog)
- ðŸ“„ **Composer Agent mode** - Agentic multi-file editing with planning
- ðŸ“„ **Context-aware architecture**:
  - Embedding and retrieval system
  - Reranking process for context prioritization
  - Priompt framework for context structuring
- ðŸ“„ Multi-model support (GPT-4, Claude 3.5 Sonnet, Gemini, custom models)
- ðŸ“„ Sub-100ms latency for completions via edge computing

**What We Can Verify** (from user experience):
- âš ï¸ Inline editing and tab completion
- âš ï¸ Complex approval workflow (users report "4+ accept buttons")
- âš ï¸ VS Code fork - maintains VS Code compatibility
- âš ï¸ Enterprise-grade security with privacy mode

**Technical Architecture** (from Cursor blog):
- **Fast-apply system**: Speculative execution for instant edits
- **Context retrieval**: Top-k chunks + reranking
- **Apply phase**: Uses specialized fast-apply model (not same as chat model)
- **Three-stage pipeline**: Query â†’ Retrieve â†’ Rerank â†’ Apply

**What We DON'T Know**:
- â“ Exact agent orchestration strategy
- â“ Memory/persistence across sessions
- â“ How complexity triggers differ between inline vs composer mode

**Sources**:
- Cursor official blog: "Instant Apply" technical deep-dive
- Morphllm analysis: "How Cursor Composer and Apply Work"
- Collabnix: "Technical Architecture" deep-dive
- User reports on forums and HN

**Competitive Advantages**:
- 1000 tokens/sec edit speed (fastest in class)
- Sub-100ms completion latency
- Context-aware across entire codebase
- Multi-model flexibility

**HONESTY CHECK**: Technical blog posts exist but are high-level. Actual implementation details (model training, exact architectures) are proprietary.

---

### 8. Windsurf (Codeium)
**Evidence Level**: ðŸ“„ DOCUMENTED (official docs + marketing data)

**What's Documented** (from official sources):
- ðŸ“„ **Cascade agentic system** - "A coding agent that works with you, not just for you"
- ðŸ“„ **Flow awareness**: Tracks file edits, terminal commands, clipboard, conversation history
  - Proprietary models built to ingest this "shared timeline"
  - Infers user intent from context
- ðŸ“„ **Usage stats** (from official website):
  - 90% of code per user written by Cascade
  - 57M lines generated by Cascade every day
- ðŸ“„ **Knowledge base integration**:
  - Deep semantic repo understanding
  - Curated docs integration
  - Centralized source of truth for best practices
- ðŸ“„ **Features**:
  - App deploys
  - Web and docs search
  - Memories & Rules system
  - Workflows
  - Model Context Protocol (MCP) support
  - DeepWiki, Codemaps (Beta), Vibe and Replace

**Technical Architecture** (from docs):
- **Context tracking**: Files viewed/edited, terminal commands, clipboard
- **Action suggestions**: Based on workflow patterns
- **Multi-level context**: Line-level, file-level, repository-level
- **VS Code fork**: Maintains VS Code compatibility

**What We Can Infer**:
- âš ï¸ Inline editing and tab completion
- âš ï¸ Terminal integration for command awareness
- âš ï¸ Agentic workflows beyond simple completion

**What We DON'T Know**:
- â“ How proprietary models differ from base models
- â“ Memory persistence across sessions
- â“ Exact inference pipeline for intent detection

**Sources**:
- Official Windsurf docs: https://docs.windsurf.com/
- Windsurf Cascade page: https://windsurf.com/cascade
- Latent Space podcast interview with founders
- Multiple educational blog posts and tutorials

**Competitive Advantages**:
- 90% code generation rate (highest claimed)
- 57M lines/day production volume (demonstrates scale)
- Flow awareness with timeline tracking (unique approach)
- MCP integration (extensibility)

**HONESTY CHECK**: Marketing numbers (90%, 57M) impressive but unverifiable. Docs are detailed but implementation is closed source.

---

### 9. Google Jules (Gemini Code Assist Agent)
**Evidence Level**: â“ UNKNOWN (very limited public info)

**What We Can Infer**:
- âš ï¸ Autonomous bug fixing (from Google I/O demo)
- âš ï¸ GitHub integration
- âš ï¸ Multi-step planning

**What We DON'T Know**:
- â“ Everything else

**Sources**:
- Google I/O announcement
- Limited blog post

**HONESTY CHECK**: Almost no information. Mostly speculation.

---

### 10. Devin (Cognition AI)
**Evidence Level**: âš ï¸ INFERRED (closed, but some demos/interviews)

**What We Can Infer** (from demos + SWE-bench results):
- âš ï¸ 13.86% SWE-bench score (verified)
- âš ï¸ Persistent memory across sessions (from demos)
- âš ï¸ Repository scanning (from demos)
- âš ï¸ Long-running autonomous tasks (from demos)

**What We DON'T Know**:
- â“ Memory implementation
- â“ Planning architecture
- â“ Tool system

**Sources**:
- SWE-bench leaderboard
- Cognition AI demos
- Blog posts

---

## Benchmark Performance Data

### SWE-bench Leaderboard (Current SOTA)
**Source**: https://www.swebench.com/

**Top Performers (Full/Verified/Lite):**
- **Grok 4**: 75% (current leader)
- **GPT-5**: 74.9%
- **Claude Opus 4.1**: 74.5%
- **Claude Haiku 4.5**: 73.3%
- **Devin (Cognition)**: 13.86% (older data)

**Datasets**:
- **SWE-bench Full**: 2,294 instances
- **SWE-bench Verified**: 500 human-filtered instances
- **SWE-bench Lite**: 300 cost-effective subset
- **SWE-bench Multimodal**: 517 visual element tasks
- **SWE-bench-Live**: 1,565 monthly-updated tasks (164 repos)

**Key Insight**: Current SOTA models at 75% on curated benchmarks, but real-world performance varies significantly.

### Terminal-Bench Leaderboard
**Source**: https://www.tbench.ai/leaderboard

**Top Performers (T-Bench-Core-v0, 80 tasks):**
1. **Ante (Antigma Labs)**: 60.3% Â± 1.1 (Claude Sonnet 4.5)
2. **Droid (Factory)**: 58.8% Â± 0.9 (Claude Opus 4.1)
3. **Claude Code**: 43.2% Â± 1.3 (Claude Opus 4)

**Benchmark Focus**:
- Software engineering tasks
- System administration
- Scientific computing
- Real terminal environment simulation

**Key Insight**: Terminal-specific agents show wide performance variance (43% to 60%), suggesting architecture matters significantly beyond model quality.

### Available Benchmarks for Aircher
**via Terminal-Bench Registry:**
- SWE-bench Verified (500 tasks)
- AppWorld (domain-specific)
- DevEval (development workflows)
- EvoEval (code evolution)

**Integration**: Terminal-Bench provides unified harness - can evaluate Aircher on all registered benchmarks via single CLI.

---

## Editor-Based Systems

### 11. GitHub Copilot
**Evidence Level**: âš ï¸ INFERRED (closed source)

**What We Know**:
- âœ… Inline completion (verified by using it)
- âœ… Chat interface (verified)
- âš ï¸ Workspace indexing (from docs)

**What We DON'T Know**:
- â“ Agent architecture (if any)
- â“ Context strategy

---

## Pattern Analysis: What's Actually Common vs Unique?

### Common Patterns (Most/All Agents Have):
1. **Multi-model support** - Universal (OpenCode: 75+ models, Cursor: 4+, Windsurf: multiple)
2. **File operations** - Read/write/edit files (all agents)
3. **Inline editing** - Editor-based agents (Cursor, Windsurf, Zed)
4. **Context from LSP** - Editor-based agents get this for free
5. **Basic tool calling** - Industry standard (all agents)
6. **VS Code fork architecture** - Cursor, Windsurf, many editors

### Documented Patterns (Multiple Agents):
1. **Plan/Build separation** - âœ… OpenCode (verified, documented)
2. **Flow/context awareness** - ðŸ“„ Windsurf Cascade (tracks edits/terminal/clipboard)
3. **Sub-agent parallelization** - ðŸ“„ Claude Code (documented feature)
4. **Fast specialized models** - ðŸ“„ Cursor fast-apply (Llama-3-70, 1000 tokens/sec)
5. **Semantic code retrieval** - ðŸ“„ Cursor (embedding + rerank), âš ï¸ not Claude Code (grep-only)

### Rare/Unique Patterns (One or Few Agents):
1. **LSP integration in CLI agents** - OpenCode (terminal-based, not editor)
2. **Git-based snapshots** - OpenCode (can cause issues with large repos)
3. **Specialized agent configs** - Factory Droid (58.8% Terminal-Bench, #2)
4. **Persistent episodic memory** - Devin (inferred), **Aircher (we have it)**
5. **Terminal command awareness** - Windsurf Cascade (proprietary)
6. **MCP protocol integration** - Windsurf (extensibility), OpenCode (supports)

### Unique to Aircher (Verified):
1. **Three-layer memory** (Episodic + Knowledge Graph + Working) - âœ… No competitor has all three
2. **Dynamic context pruning with relevance scoring** - âœ… Not documented elsewhere
3. **ACP protocol multi-frontend** - âœ… Zed has ACP, but they're an editor, not agent backend
4. **60% tool call reduction** - âš ï¸ POC result, needs production validation

### Patterns We Borrowed (With Verification Status):
1. **Plan/Build modes** - âœ… From OpenCode (verified docs)
2. **LSP integration** - âœ… From OpenCode/editors (common pattern)
3. **Event bus** - âš ï¸ Inferred from OpenCode (not verified in code)
4. **Git snapshots** - âœ… From OpenCode (verified in issues)
5. **Model routing** - ðŸ“„ From Amp (documented but no implementation details)

---

## What We Can Confidently Claim

### âœ… VERIFIED UNIQUE (We Can Prove):
1. **Three-layer memory architecture** - No competitor has Episodic + Knowledge Graph + Working Memory combined
2. **ACP-native agent backend** - Zed has ACP, but they're an editor; we're a dedicated agent backend
3. **Dynamic context pruning with relevance scoring** - Not documented in any competitor
4. **Rust-based agent performance** - Performance advantage (vs Python competitors)

### ðŸ“„ INSPIRED BY VERIFIED PATTERNS (Can Claim Inspiration):
1. **Plan/Build modes** - âœ… From OpenCode (verified docs at opencode.ai/docs/modes/)
2. **LSP integration** - âœ… From OpenCode/editors (documented pattern)
3. **Git snapshots** - âœ… From OpenCode (verified in GitHub issues)
4. **Event-driven architecture** - âš ï¸ Inferred from OpenCode/Windsurf patterns
5. **Multi-model routing** - ðŸ“„ From Amp (documented concept)

### âš ï¸ CANNOT CLAIM WITHOUT BENCHMARKS:
1. **"Better than Claude Code"** - Need head-to-head Terminal-Bench/SWE-bench comparison
2. **"Better than Factory Droid"** - They're #2 on Terminal-Bench (58.8%), we're untested
3. **"60% tool reduction"** - POC validated, but needs production validation
4. **"90% research improvement with sub-agents"** - Not implemented yet
5. **"50% error reduction via LSP"** - Not measured yet
6. **"40% cost reduction via routing"** - Not implemented or measured

### Current Competitive Position (Honest Assessment):
**What We Know**:
- **OpenCode**: 28.8k stars, verified open source, Plan/Build modes work
- **Claude Code**: 43.2% Terminal-Bench, grep-only (40% token overhead), no memory
- **Factory Droid**: 58.8% Terminal-Bench (#2), closed source
- **Cursor**: 1000 tokens/sec edits, fast-apply model, closed source
- **Windsurf**: 90% code generation rate (claimed), flow awareness, closed source

**Where Aircher Stands**:
- âœ… **Unique**: Three-layer memory (no competitor has this)
- âœ… **Unique**: Dynamic context pruning algorithm
- âœ… **Competitive**: ACP multi-frontend support
- âš ï¸ **Untested**: No benchmark scores yet (need Terminal-Bench/SWE-bench runs)
- âš ï¸ **Partial**: Core patterns implemented (Week 7 Days 1-5), routing pending

**To Be Truly SOTA**: Must score >58.8% on Terminal-Bench (beat Factory Droid) or >75% on SWE-bench (match current SOTA)

---

## Action Items: Research We Need to Do

### Critical (Must Do Before Claiming SOTA):
1. [ ] **Find OpenCode source code** - Is it actually open source?
2. [ ] **Run benchmarks vs Claude Code** - Prove our claims with data
3. [ ] **Test Cursor/Windsurf** - Understand their actual capabilities
4. [ ] **Analyze Zed source code** - Learn from their ACP implementation
5. [ ] **Study Sweep source code** - Understand their planning system

### Important (Should Do):
1. [ ] **Monitor SWE-bench leaderboard** - Track Factory Droid, Devin scores
2. [ ] **Collect Claude Code user feedback** - Validate sub-agent waste claims
3. [ ] **Sourcegraph Amp docs deep-dive** - Understand model routing
4. [ ] **Jules documentation** - Track Google's approach when public

### Research Questions:
1. Is LSP integration unique, or do all CLI agents have it?
2. Do other agents use event-driven architectures internally?
3. Are we the only ones with persistent memory?
4. What patterns are we missing that competitors have?

---

## Honest Assessment: Where We Stand

### What We KNOW We're Good At:
- âœ… Memory systems (episodic + knowledge graph + working memory)
- âœ… Rust implementation (performance advantage)
- âœ… ACP protocol (multi-frontend support)

### What We THINK We're Good At (Need to Prove):
- âš ï¸ Dynamic context management
- âš ï¸ Tool call reduction (60% in POC)
- âš ï¸ Continuous work without restart

### What We DON'T Know:
- â“ How we compare to Claude Code (no benchmarks)
- â“ How we compare to Factory Droid (#1 on Terminal-Bench)
- â“ Whether our architecture is actually better
- â“ If anyone else has similar patterns we don't know about

---

## Next Steps: Becoming Truly SOTA

### Week 8-9: Competitive Research
1. Find and read ALL open source agent code
2. Try ALL major competitors (Claude Code, Cursor, Windsurf)
3. Run SWE-bench tasks against them
4. Document what they actually do (not what we think they do)

### Week 9: Benchmarks
1. Run same tasks in Aircher vs Claude Code vs Cursor
2. Measure: tool calls, context efficiency, success rate, cost
3. Get empirical evidence for our claims
4. Publish honest comparison

### Week 10: Research Paper
1. Only claim what we can prove
2. Mark inference vs verification clearly
3. Focus on our unique contributions (memory systems)
4. Be honest about limitations

---

## Conclusion

**What We Got Right** (After Research):
1. âœ… **Memory systems are genuinely unique** - No competitor has all three layers
2. âœ… **ACP-native backend is valuable** - Multi-frontend support without being an editor
3. âœ… **Rust implementation provides advantage** - Performance matters for benchmarks
4. âœ… **Research-driven architecture** - Borrowed proven patterns (OpenCode, Cursor, Windsurf)

**What We Learned from Research**:
1. **OpenCode is real** - 28.8k stars, open source, verified patterns we can study
2. **Claude Code's weaknesses are verified** - Grep-only (40% overhead), no memory, 43.2% Terminal-Bench
3. **Factory Droid is strong** - 58.8% Terminal-Bench, our target to beat
4. **Cursor has technical edge** - 1000 tokens/sec edits via specialized models
5. **Windsurf has scale** - 90% generation rate, 57M lines/day (if numbers are real)
6. **Benchmarks are accessible** - Terminal-Bench registry provides easy evaluation

**What We Need to Do**:
1. â— **Run Terminal-Bench evaluation** - Get baseline score (critical for credibility)
2. â— **Run SWE-bench Verified (500 tasks)** - Compare against 75% SOTA
3. âš ï¸ **Finish Week 7-8 implementation** - Complete model router, specialized agents
4. âš ï¸ **Validate memory advantage** - Prove 60% tool reduction in production
5. ðŸ“Š **Document everything** - Evidence-based claims only

**Honest Positioning**:
- **NOT**: "Better than Claude Code" (can't prove without benchmarks)
- **YES**: "Inspired by OpenCode, with unique three-layer memory"
- **NOT**: "SOTA performance" (need to prove with scores >58.8% Terminal-Bench)
- **YES**: "Research-driven hybrid architecture combining proven patterns"

**Research Principle** (Updated):
> "Verify before claiming. Document evidence levels. Run benchmarks before superiority claims. Honest about what we don't know."

**Next Steps**: Run benchmarks (Week 9), then we can make evidence-based claims.
